import gym
import numpy as np
from gym.wrappers.monitoring.video_recorder import VideoRecorder
from base64 import b64encode
from IPython.display import HTML

# Create environment
env = gym.make("FrozenLake-v1")

# Reset environment
env.reset()

# Sample 20 steps
for i in range(20):
    observation, reward, done, info = env.step(env.action_space.sample()) 
    print("step", i, observation, reward, done, info)

env.close()

# Install necessary packages
!apt-get install -y xvfb x11-utils
!pip install pyvirtualdisplay==0.2.*
from pyvirtualdisplay import Display
display = Display(visible=False, size=(1400, 900))
_ = display.start()

# Record video before training
before_training = "frozenlake_before_training.mp4"
video = VideoRecorder(env, before_training)

# Reset environment
env.reset()

# Run random actions and record video
for i in range(200):
    env.render()
    video.capture_frame()
    observation, reward, done, info = env.step(env.action_space.sample())

video.close()
env.close()

# Function to render MP4
def render_mp4(videopath: str) -> str:
    mp4 = open(videopath, 'rb').read()
    base64_encoded_mp4 = b64encode(mp4).decode()
    return f'<video width=400 controls><source src="data:video/mp4;' \
           f'base64,{base64_encoded_mp4}" type="video/mp4"></video>'


# Display video
html = render_mp4(before_training)
HTML(html)

import gym
import numpy as np
from gym.wrappers.monitoring.video_recorder import VideoRecorder
from base64 import b64encode
from IPython.display import HTML

# Create FrozenLake environment
env = gym.make("FrozenLake-v1")

# Initialize Q-table with zeros
action_space_size = env.action_space.n
state_space_size = env.observation_space.n
q_table = np.zeros((state_space_size, action_space_size))

# Set hyperparameters
num_episodes = 10000
max_steps_per_episode = 100
learning_rate = 0.1
discount_rate = 0.99
exploration_rate = 1
max_exploration_rate = 1
min_exploration_rate = 0.01
exploration_decay_rate = 0.001

# Q-learning algorithm
for episode in range(num_episodes):
    state = env.reset()
    done = False
    
    for step in range(max_steps_per_episode):
        # Exploration-exploitation trade-off
        exploration_threshold = np.random.uniform(0, 1)
        if exploration_threshold > exploration_rate:
            action = np.argmax(q_table[state, :])  # Exploitation
        else:
            action = env.action_space.sample()  # Exploration
        
        # Take action and observe next state and reward
        new_state, reward, done, _ = env.step(action)
        
        # Update Q-table using Bellman equation
        q_table[state, action] = q_table[state, action] * (1 - learning_rate) + \
                                  learning_rate * (reward + discount_rate * np.max(q_table[new_state, :]))
        
        # Transition to next state
        state = new_state
        
        if done:
            break
    
    # Decay exploration rate
    exploration_rate = min_exploration_rate + \
                        (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate * episode)
    
    if (episode + 1) % 100 == 0:
        print(f"Episode {episode + 1}/{num_episodes}, Exploration rate: {exploration_rate}")

# Record video after training
after_training = "frozenlake_after_training.mp4"
video = VideoRecorder(env, after_training)

state = env.reset()
done = False
while not done:
    action = np.argmax(q_table[state, :])
    state, _, done, _ = env.step(action)
    video.capture_frame()

video.close()
env.close()

# Function to render MP4
def render_mp4(videopath: str) -> str:
    mp4 = open(videopath, 'rb').read()
    base64_encoded_mp4 = b64encode(mp4).decode()
    return f'<video width=400 controls><source src="data:video/mp4;' \
           f'base64,{base64_encoded_mp4}" type="video/mp4"></video>'

# Display video
html = render_mp4(after_training)
HTML(html)
